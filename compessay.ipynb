{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Language Differences Between Interest Groups\n",
    "**Aditi Vinod & Luke Witten**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: []\n",
    "\n",
    "### Primary Questions\n",
    "What terms are unique to internet sub-communities, like gamers? \\\n",
    "Based on social media content or messages sent, can an individual be associated with a specific internet sub-community?\n",
    "\n",
    "### Background Information\n",
    "Due to the ease of access to the internent, people have been able to find and interact with niche groups that share similar interests to themselves online. Similar to how dialects and accents have formed based on regions in the past, various new words and phrases, exclusive to individual communities, have popped up in sectors of the internet. \n",
    "\n",
    "One example of a popular community with a large online presence is gamers. Especially following the recent 2020 COVID-19 pandemic, there has been a significant increase in the past few years in the number of individuals who spend time playing games - both individually and in teams - online. Through content creators (on Twitch, YouTube, TikTok, etc.), online forums, and memes, the English-speaking gaming community likely has one of the most developed subsets of the language to exist; there are a significant number of terms, like \"pog,\" \"gg,\" and \"smurf\" that have either no significance or mean something different to non-gaming members of society. \n",
    "\n",
    "Being able to identify these differences in language helps []\n",
    "\n",
    "In order to create a list of \"gamer words,\" or terms that are unique to gamers online, and determine whether an individual is a gamer, several steps must be taken: \\\n",
    "1-Collect data that reflects usage of langauge online in \"gamer\" communities and \"normally\" \\\n",
    "2-Compare the two data sets in order to create a list of \"gamer words\" that did not include unaviodable common words in the English language \\\n",
    "3-Collect data for individual users representing their usage of language \\\n",
    "4-Use the \"gamer words\" list and the individuals' usage of said gamer words to determine whether they are a gamer or not\n",
    "\n",
    "\n",
    "\n",
    "1-What is the question you are trying to answer or the story that you are trying you tell?\n",
    "2-Why is this question or story important?\n",
    "3-What were the main steps your project made towards answering the question or telling the story?\n",
    "\n",
    "### Prerequisite Code\n",
    "The following cell ensures that the necessary functions are imported into the document and that the notebook uses the latest version of all code in the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "from scrape_data import *\n",
    "\n",
    "import csv\n",
    "import gamer_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Data Collection\n",
    "The data that was collected in order to determine \"gamer\" and \"normal\" usage of language on the internet was pulled from Reddit. Reddit was chosen because it is a platform that is used by a significant amount of people on the internet accross hundreds of sub-communities. \n",
    "\n",
    "1-Where did you get your data from?\n",
    "2-How did you get this data (i.e., did you programmatically download it or did you access it through an API)?\n",
    "3-How did you store and/or process this data (e.g., did you store and process it in Pandas)?\n",
    "4-What information did you get from this data that you used in the presentation of your results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data (conveniently stored as CSV's) we can actually start analyzing our data.\n",
    "\n",
    "Storing our data in this form means that we do not need to re-scrape the data from Reddit every time we want to analyze it, but it also means that the data is not readily accessible by the computer.\n",
    "\n",
    "Luckily reading data from a CSV is not difficult, using the function `csv_to_dict` we can easily convert from a CSV file to a dictionary in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the dictionary is 17752\n"
     ]
    }
   ],
   "source": [
    "def csv_to_dict(file_name):\n",
    "    with open(file_name) as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        word_dict = dict(reader)\n",
    "    return word_dict\n",
    "\n",
    "gamer_dictionary = csv_to_dict(\"gaming.csv\")\n",
    "for word in gamer_dictionary:\n",
    "    value = gamer_dictionary[word]\n",
    "    gamer_dictionary[word] = int(value)\n",
    "\n",
    "\n",
    "print(f\"The length of the dictionary is {len(gamer_dictionary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have access to a dictionary that tells us how many times a word is used in the dataset we collected, but many of these words appear only once or are typos. These results are not particularly useful as we want words that are commonly used by gamers.\n",
    "\n",
    "We can remove words from the dataset who do not show up enough times fairly simply using `remove_too_uncommon()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the dictionary is 5241.\n"
     ]
    }
   ],
   "source": [
    "# Create a new dictionary with only words that appear 3 or more times\n",
    "gamer_dictionary_1 = gamer_words.remove_too_uncommon(gamer_dictionary.copy(),3)\n",
    "print(f\"The length of the dictionary is {len(gamer_dictionary_1)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is much smaller than the original and likely more representative of words that gamers actually say. \n",
    "\n",
    "If we want to find out which words gamers use most, then all we need to do is find which words appear most frequently in the dataset. Once we have these \"gamer words\" we can compare them against a user's post history to find out if they are a gamer or not.\n",
    "\n",
    "Let's run `find_most_frequent()` to find the 5 most frequently occuring \"gamer words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The five most frequent gamer words are {'the': 6797, 'to': 4351, 'and': 3528, 'a': 3472, 'of': 3008}.\n"
     ]
    }
   ],
   "source": [
    "gamer_dictionary_2 = gamer_words.find_most_frequent(gamer_dictionary_1.copy(),5)\n",
    "print(f\"The five most frequent gamer words are {gamer_dictionary_2}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the words that appeared we can see that something is obviously wrong. While there is no doubt that gamers use words like \"a\", \"and\", and \"the\" frequently, nobody would be fooled into believing that these words are unique to the gamer vocabulary. \n",
    "\n",
    "To fully determine what words are unique to the gamer vocabulary, we will need to somehow compare the language dataset from gamers to a language dataset for non-gamers.\n",
    "\n",
    "For our convenience, this data has already been scraped and is stored in `normal_dictionary.csv`\n",
    "\n",
    "~ aditi put something here about our methodology from scraping normal subreddits and what normal subreddits we scraped from~\n",
    "\n",
    "To filter out words from both dictionaries we can go through each word in the gamer dictionary and compare how frequently a word occurrs in both dictionaries. If a word is used too similarly in both language sets then we can remove the word from both. \n",
    "\n",
    "Unfortunately, our dictionary currently stores the number of times a word has been used in a language set and not the percentage of times that a word is used in the entire language set. This is a simple enough thing to code though, and now exists in the form of the `instances_to_decimal` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"the\" is used 6797 times in the gamer dictionary.\n",
      "\"the\" is used 0.047734758517041107 of the time in the gamer language set.\n"
     ]
    }
   ],
   "source": [
    "# Create and store the normal dictionary\n",
    "normal_dictionary = csv_to_dict(\"normal.csv\")\n",
    "for word in normal_dictionary:\n",
    "    value = normal_dictionary[word]\n",
    "    normal_dictionary[word] = int(value)\n",
    "\n",
    "# Remove infrequent words\n",
    "normal_dictionary_1 = gamer_words.remove_too_uncommon(normal_dictionary.copy(),3)\n",
    "\n",
    "gamer_decimal_dictionary = gamer_words.instances_to_decimal(gamer_dictionary_1.copy())\n",
    "normal_decimal_dictionary = gamer_words.instances_to_decimal(normal_dictionary_1.copy())\n",
    "\n",
    "the_usages = gamer_dictionary_1[\"the\"]\n",
    "the_decimal = gamer_decimal_dictionary[\"the\"]\n",
    "print(f\"\\\"the\\\" is used {the_usages} times in the gamer dictionary.\")\n",
    "print(f\"\\\"the\\\" is used {the_decimal} of the time in the gamer language set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data in a usable form, we can parse through our two language sets and remove words that appear a similar percentage of the time in both data sets. While the actual percentage value that is used for this process is arbitrary, we found that removing words with frequency values within +- 15% of each other worked well to find a good set of words.\n",
    "\n",
    "We can use the function `remove_most_common()` to parse our two dictionaries with respect to each other and even output a list of words that were removed from both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the dictionary is 4837.\n"
     ]
    }
   ],
   "source": [
    "normal_decimal_dictionary_1, gamer_decimal_dictionary_1, ignore_list =gamer_words.remove_most_common(normal_decimal_dictionary.copy(),gamer_decimal_dictionary.copy())\n",
    "print(f\"The length of the dictionary is {len(gamer_decimal_dictionary_1)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we now have two curated language sets, one for the gamer language set and one for the normal language set, we still do not have a method for determining what words are extremely specific to the gamer lexicon. \n",
    "\n",
    "To do this, we can examine all words that appear in both the gamer and the normal dataset, if the word is used 5 times more frequently in the gamer language set than the normal language set, then we can determine it is a gamer word. While this 5x threshold is undoubtedly arbitrary, in testing we found that this value produced a good number of gamer words that were not too over specific, but also not too common as to not be considered gamer specific.\n",
    "\n",
    "Sometimes possible gamer words do not appear at all in the normal language set, so there needs to be a method for these words to become gamer words as well. We foudn that in testing, if a word does not appear in both data sets and its frequency of occurrences values is over .00005 then it can be reasonably considered a gamer word. While this value of .00005 is also arbitrary, we found that using it produces a solid set of gamer words.\n",
    "\n",
    "Using our curated data sets, we can find a number of gamer words using the `determine_gamer_words()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 966 gamer words.\n"
     ]
    }
   ],
   "source": [
    "gamer_words_1 = gamer_words.determine_gamer_words(normal_decimal_dictionary_1.copy(),gamer_decimal_dictionary.copy())\n",
    "\n",
    "print(f\"There are {len(gamer_words_1)} gamer words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Visualization\n",
    "\n",
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d59e5a3676c8f6014a0bf8a6d620411eb6abbd5536b79329eb052434920fc55"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
